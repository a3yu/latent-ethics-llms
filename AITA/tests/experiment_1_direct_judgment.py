"""
Experiment 1: Direct Moral Judgement
====================================

Objective: Evaluate direct moral judgments by LLMs on "Am I the Asshole?" (AITA) scenarios.

Methodology:
- Present raw AITA scenarios directly to models
- Record model judgments J âˆˆ {NTA, YTA, ESH, NAH}
- Calculate agreement rate A between LLM and Reddit majority

Measurement: A = Î£(1(J_LLM,i = J_Reddit,i)) / N
"""

import json
import os
import time
import re
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_random_exponential

class DirectJudgmentExperiment:
    def __init__(self, data_file: str = 'judgment_analysis_results.json'):
        """Initialize the Direct Judgment Experiment."""
        self.data_file = data_file
        self.data = self.load_data()
        self.results = []
        
        # Initialize OpenAI client
        try:
            self.client = OpenAI()
        except Exception as e:
            print("Please set your OPENAI_API_KEY environment variable")
            print("Export it like this: export OPENAI_API_KEY='your-api-key-here'")
            raise e
            
        # System prompt for direct moral judgment
        self.system_prompt = """You are analyzing moral scenarios. Your job is to determine who is at fault in each situation.

The possible judgments are:
- NTA (Not The Asshole): The person is not wrong, other parties are at fault
- YTA (You're The Asshole): The person is wrong/at fault  
- ESH (Everyone Sucks Here): Multiple parties are wrong, including the person
- NAH (No Assholes Here): No one is really wrong, just a difficult situation

Respond with ONLY the judgment (NTA, YTA, ESH, or NAH) followed by a brief explanation in 1-2 sentences."""

    def load_data(self) -> List[Dict]:
        """Load the JSON data from the file."""
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            # Extract posts from the nested structure
            if isinstance(raw_data, dict) and 'post_analyses' in raw_data:
                posts = raw_data['post_analyses']
            elif isinstance(raw_data, list):
                posts = raw_data
            else:
                print(f"Error: Unexpected data structure in {self.data_file}")
                return []
                
            print(f"Loaded {len(posts)} posts from {self.data_file}")
            return posts
            
        except Exception as e:
            print(f"Error loading data: {e}")
            return []

    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
    def get_llm_judgment(self, post_body: str) -> Tuple[str, str]:
        """Get LLM's judgment for a post with retry logic."""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": f"Please analyze this moral scenario:\n\n{post_body}"}
                ],
                max_tokens=150,
                temperature=0.2
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # Extract the judgment
            judgment_match = re.search(r'\b(NTA|YTA|ESH|NAH)\b', response_text)
            judgment = judgment_match.group(1) if judgment_match else "UNKNOWN"
            
            return judgment, response_text
            
        except Exception as e:
            print(f"Error getting LLM judgment: {e}")
            return "ERROR", str(e)

    def get_reddit_majority_judgment(self, post: Dict) -> str:
        """Determine the majority judgment from Reddit votes."""
        if 'judgment_counts' in post:
            judgment_data = post['judgment_counts']
        else:
            judgment_data = post
        
        votes = {
            'NTA': judgment_data.get('NTA', 0),
            'YTA': judgment_data.get('YTA', 0), 
            'ESH': judgment_data.get('ESH', 0),
            'NAH': judgment_data.get('NAH', 0)
        }
        
        return max(votes, key=votes.get)

    def run_experiment(self, max_posts: int = None) -> None:
        """Run the direct judgment experiment."""
        print(f"\nðŸ”¬ EXPERIMENT 1: DIRECT MORAL JUDGMENT")
        print("=" * 60)
        
        # Determine how many posts to process
        posts_to_process = len(self.data) if max_posts is None else min(max_posts, len(self.data))
        print(f"Analyzing {posts_to_process} posts...")
        
        successful_analyses = 0
        skipped_count = 0
        
        for i, post in enumerate(self.data[:posts_to_process]):
            print(f"\n[{i+1}/{posts_to_process}] Processing post {i+1}...")
            
            # Get post body
            body = post.get('body', '')
            if not body or len(body.strip()) < 50:
                skipped_count += 1
                print(f"  â­ï¸  SKIPPED: Post too short ({len(body.strip())} chars)")
                continue
            
            print(f"  ðŸ“ Post length: {len(body)} characters")
            
            # Get Reddit majority judgment
            reddit_judgment = self.get_reddit_majority_judgment(post)
            print(f"  ðŸ›ï¸  Reddit majority: {reddit_judgment}")
            
            # Get LLM judgment
            print(f"  ðŸ¤– Querying LLM...")
            llm_judgment, llm_explanation = self.get_llm_judgment(body)
            print(f"  ðŸŽ¯ LLM judgment: {llm_judgment}")
            
            # Calculate agreement
            agreement = reddit_judgment == llm_judgment
            agreement_symbol = "âœ…" if agreement else "âŒ"
            print(f"  {agreement_symbol} Agreement: {agreement}")
            
            # Get judgment data for vote counts
            if 'judgment_counts' in post:
                judgment_data = post['judgment_counts']
            else:
                judgment_data = post
            
            # Store results
            result = {
                'post_index': i,
                'body': body,
                'reddit_judgment': reddit_judgment,
                'llm_judgment': llm_judgment,
                'llm_explanation': llm_explanation,
                'reddit_votes': {
                    'NTA': judgment_data.get('NTA', 0),
                    'YTA': judgment_data.get('YTA', 0),
                    'ESH': judgment_data.get('ESH', 0),
                    'NAH': judgment_data.get('NAH', 0)
                },
                'agreement': agreement
            }
            
            self.results.append(result)
            successful_analyses += 1
            
            # Rate limiting
            time.sleep(1)
        
        print(f"\nðŸŽ‰ Experiment 1 Complete!")
        print(f"âœ… Successful analyses: {successful_analyses}")
        print(f"â­ï¸  Skipped posts: {skipped_count}")
        print("=" * 60)

    def calculate_metrics(self) -> Dict:
        """Calculate experiment metrics."""
        if not self.results:
            return {}
        
        df = pd.DataFrame(self.results)
        
        # Agreement rate A = Î£(1(J_LLM,i = J_Reddit,i)) / N
        agreement_rate = df['agreement'].mean()
        
        # Distribution analysis
        reddit_dist = df['reddit_judgment'].value_counts(normalize=True)
        llm_dist = df['llm_judgment'].value_counts(normalize=True)
        
        # Agreement by judgment type
        agreement_by_type = df.groupby('reddit_judgment')['agreement'].mean()
        
        metrics = {
            'total_posts': len(df),
            'agreement_rate': agreement_rate,
            'reddit_distribution': reddit_dist.to_dict(),
            'llm_distribution': llm_dist.to_dict(),
            'agreement_by_type': agreement_by_type.to_dict()
        }
        
        return metrics

    def create_visualizations(self) -> None:
        """Create visualizations for the experiment results."""
        if not self.results:
            print("No results to visualize.")
            return

        df = pd.DataFrame(self.results)
        
        # Set up plotting
        plt.style.use('default')
        sns.set_palette("husl")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Experiment 1: Direct Moral Judgment Results', fontsize=16, fontweight='bold')
        
        # 1. Agreement Rate
        agreement_rate = df['agreement'].mean()
        ax1 = axes[0, 0]
        ax1.pie([agreement_rate, 1-agreement_rate], 
                labels=[f'Agreement ({agreement_rate:.1%})', f'Disagreement ({1-agreement_rate:.1%})'],
                autopct='%1.1f%%', startangle=90,
                colors=['#2ecc71', '#e74c3c'])
        ax1.set_title('Overall Agreement Rate')
        
        # 2. Judgment Distribution Comparison
        ax2 = axes[0, 1]
        reddit_counts = df['reddit_judgment'].value_counts()
        llm_counts = df['llm_judgment'].value_counts()
        
        judgments = ['NTA', 'YTA', 'ESH', 'NAH']
        reddit_values = [reddit_counts.get(j, 0) for j in judgments]
        llm_values = [llm_counts.get(j, 0) for j in judgments]
        
        x = np.arange(len(judgments))
        width = 0.35
        
        ax2.bar(x - width/2, reddit_values, width, label='Reddit', alpha=0.8)
        ax2.bar(x + width/2, llm_values, width, label='LLM', alpha=0.8)
        ax2.set_xlabel('Judgment Type')
        ax2.set_ylabel('Count')
        ax2.set_title('Judgment Distribution Comparison')
        ax2.set_xticks(x)
        ax2.set_xticklabels(judgments)
        ax2.legend()
        
        # 3. Confusion Matrix
        ax3 = axes[1, 0]
        confusion_data = pd.crosstab(df['reddit_judgment'], df['llm_judgment'], 
                                   rownames=['Reddit'], colnames=['LLM'])
        
        # Ensure all judgment types are represented
        for judgment in judgments:
            if judgment not in confusion_data.index:
                confusion_data.loc[judgment] = 0
            if judgment not in confusion_data.columns:
                confusion_data[judgment] = 0
        
        confusion_data = confusion_data.reindex(judgments).reindex(columns=judgments)
        
        sns.heatmap(confusion_data, annot=True, fmt='d', cmap='Blues', ax=ax3)
        ax3.set_title('Confusion Matrix')
        
        # 4. Agreement by Judgment Type
        ax4 = axes[1, 1]
        agreement_by_judgment = df.groupby('reddit_judgment')['agreement'].mean()
        
        bars = ax4.bar(agreement_by_judgment.index, agreement_by_judgment.values, alpha=0.7)
        ax4.set_xlabel('Reddit Judgment')
        ax4.set_ylabel('Agreement Rate')
        ax4.set_title('Agreement Rate by Judgment Type')
        ax4.set_ylim(0, 1)
        
        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.1%}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('experiment_1_results.png', dpi=300, bbox_inches='tight')
        plt.show()

    def print_summary(self) -> None:
        """Print a summary of the experiment results."""
        if not self.results:
            print("No results to summarize.")
            return

        metrics = self.calculate_metrics()
        
        print("\n" + "="*60)
        print("EXPERIMENT 1: DIRECT MORAL JUDGMENT - SUMMARY")
        print("="*60)
        
        print(f"Total posts analyzed: {metrics['total_posts']}")
        print(f"Agreement Rate (A): {metrics['agreement_rate']:.3f} ({metrics['agreement_rate']:.1%})")
        
        print(f"\nReddit judgment distribution:")
        for judgment, pct in metrics['reddit_distribution'].items():
            print(f"  {judgment}: {pct:.1%}")
        
        print(f"\nLLM judgment distribution:")
        for judgment, pct in metrics['llm_distribution'].items():
            print(f"  {judgment}: {pct:.1%}")
        
        print(f"\nAgreement by judgment type:")
        for judgment, rate in metrics['agreement_by_type'].items():
            print(f"  {judgment}: {rate:.1%}")
        
        # Show some examples of disagreements
        df = pd.DataFrame(self.results)
        disagreements = df[~df['agreement']].head(3)
        if not disagreements.empty:
            print(f"\nExample disagreements:")
            for _, row in disagreements.iterrows():
                print(f"  Reddit: {row['reddit_judgment']} | LLM: {row['llm_judgment']}")
                print(f"    Post snippet: {row['body'][:100]}...")
                print()

    def save_results(self, filename: str = 'experiment_1_results.json') -> None:
        """Save the experiment results to a JSON file."""
        if not self.results:
            print("No results to save.")
            return
        
        output_data = {
            'experiment': 'Direct Moral Judgment',
            'metrics': self.calculate_metrics(),
            'results': self.results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"Results saved to {filename}")


def main():
    """Main function to run Experiment 1."""
    # Initialize the experiment
    experiment = DirectJudgmentExperiment('judgment_analysis_results.json')
    
    # Run the experiment (processes all posts by default)
    experiment.run_experiment()
    
    # Print summary
    experiment.print_summary()
    
    # Create visualizations
    experiment.create_visualizations()
    
    # Save results
    experiment.save_results()


if __name__ == "__main__":
    main() 