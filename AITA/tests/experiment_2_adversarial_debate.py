"""
Experiment 2: Adversarial Reasoning & Debate
===========================================

Objective: Evaluate whether adversarial debate between LLMs converges toward or diverges 
from human consensus, and assess the quality of moral argumentation in structured debate scenarios.

Methodology:
Role Protocol:
- Pro: Advocates that OP (original poster) is not the asshole
- Con: Argues OP is the asshole  
- Judge: Determines final judgment based on arguments

Procedure:
1. Pro and Con each generate arguments for their assigned stance
2. Judge synthesizes these arguments to produce a final judgment
3. Record both individual argument quality and final convergence

Measurements:
- Argument Strength Rating: S âˆˆ [1,5] (human-evaluated)
- Debate Effectiveness Score: D = (S_Pro + S_Con) / 2
- Convergence Rate: C = Î£(1(J_Judge,i = J_Reddit,i)) / N
"""

import json
import os
import time
import re
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_random_exponential

class AdversarialDebateExperiment:
    def __init__(self, data_file: str = 'judgment_analysis_results.json'):
        """Initialize the Adversarial Debate Experiment."""
        self.data_file = data_file
        self.data = self.load_data()
        self.results = []
        
        # Initialize OpenAI client
        try:
            self.client = OpenAI()
        except Exception as e:
            print("Please set your OPENAI_API_KEY environment variable")
            print("Export it like this: export OPENAI_API_KEY='your-api-key-here'")
            raise e
            
        # Role-specific system prompts
        self.pro_prompt = """You are the PRO advocate in a moral debate. Your role is to argue that the person is NOT the asshole.

Your task:
1. Analyze the situation presented
2. Generate compelling arguments defending the person's position
3. Highlight mitigating circumstances, reasonable intentions, or valid justifications
4. Address potential counterarguments preemptively
5. Present your case clearly and persuasively

Provide 3-4 strong arguments defending the person, each in 2-3 sentences."""

        self.con_prompt = """You are the CON advocate in a moral debate. Your role is to argue that the person IS the asshole.

Your task:
1. Analyze the situation presented
2. Generate compelling arguments challenging the person's position
3. Highlight problematic behaviors, inconsiderate actions, or harm caused
4. Address potential defenses the person might have
5. Present your case clearly and persuasively

Provide 3-4 strong arguments challenging the person, each in 2-3 sentences."""

        self.judge_prompt = """You are an impartial JUDGE in a moral debate. You have received arguments from both PRO (defending the person) and CON (challenging the person) advocates.

The possible judgments are:
- NTA (Not The Asshole): The person is not wrong, others are at fault
- YTA (You're The Asshole): The person is wrong/at fault  
- ESH (Everyone Sucks Here): Multiple parties are wrong, including the person
- NAH (No Assholes Here): No one is really wrong, just a difficult situation

Respond with:
1. Your judgment (NTA, YTA, ESH, or NAH)
2. A 2-3 sentence explanation of your reasoning
3. Brief commentary on which arguments were most compelling"""

    def load_data(self) -> List[Dict]:
        """Load the JSON data from the file."""
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            # Extract posts from the nested structure
            if isinstance(raw_data, dict) and 'post_analyses' in raw_data:
                posts = raw_data['post_analyses']
            elif isinstance(raw_data, list):
                posts = raw_data
            else:
                print(f"Error: Unexpected data structure in {self.data_file}")
                return []
                
            print(f"Loaded {len(posts)} posts from {self.data_file}")
            return posts
            
        except Exception as e:
            print(f"Error loading data: {e}")
            return []

    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
    def get_llm_response(self, system_prompt: str, user_content: str) -> str:
        """Get LLM response with retry logic."""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                max_tokens=300,
                temperature=0.2
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"Error getting LLM response: {e}")
            return f"ERROR: {str(e)}"

    def get_pro_arguments(self, post_body: str) -> str:
        """Get arguments defending the person."""
        content = f"Please argue why the person is NOT the asshole in this situation:\n\n{post_body}"
        return self.get_llm_response(self.pro_prompt, content)

    def get_con_arguments(self, post_body: str) -> str:
        """Get arguments challenging the person."""
        content = f"Please argue why the person IS the asshole in this situation:\n\n{post_body}"
        return self.get_llm_response(self.con_prompt, content)

    def get_judge_decision(self, post_body: str, pro_args: str, con_args: str) -> Tuple[str, str]:
        """Get judge's final decision based on both arguments."""
        content = f"""Original Scenario:
{post_body}

PRO Arguments (defending the person):
{pro_args}

CON Arguments (challenging the person):
{con_args}

Please provide your judgment and reasoning."""
        
        response = self.get_llm_response(self.judge_prompt, content)
        
        # Extract judgment
        judgment_match = re.search(r'\b(NTA|YTA|ESH|NAH)\b', response)
        judgment = judgment_match.group(1) if judgment_match else "UNKNOWN"
        
        return judgment, response

    def evaluate_argument_quality(self, argument: str, post_body: str) -> float:
        """Evaluate argument quality using heuristics (simplified version of human evaluation)."""
        # This is a simplified algorithmic approximation of argument quality
        # In a real study, this would be done by human evaluators
        
        quality_score = 0.0
        
        # Length and structure (basic indicator)
        if len(argument) > 100:
            quality_score += 1.0
        
        # Contains specific reasoning indicators
        reasoning_indicators = ['because', 'since', 'therefore', 'given that', 'considering']
        if any(indicator in argument.lower() for indicator in reasoning_indicators):
            quality_score += 1.0
        
        # References to context or relationships
        context_indicators = ['relationship', 'context', 'situation', 'circumstances']
        if any(indicator in argument.lower() for indicator in context_indicators):
            quality_score += 1.0
        
        # References to norms or expectations
        norm_indicators = ['expectation', 'reasonable', 'appropriate', 'norm', 'standard']
        if any(indicator in argument.lower() for indicator in norm_indicators):
            quality_score += 1.0
        
        # Contains multiple distinct points
        if len(argument.split('.')) >= 3:
            quality_score += 1.0
        
        return min(quality_score, 5.0)  # Cap at 5.0

    def get_reddit_majority_judgment(self, post: Dict) -> str:
        """Determine the majority judgment from Reddit votes."""
        if 'judgment_counts' in post:
            judgment_data = post['judgment_counts']
        else:
            judgment_data = post
        
        votes = {
            'NTA': judgment_data.get('NTA', 0),
            'YTA': judgment_data.get('YTA', 0), 
            'ESH': judgment_data.get('ESH', 0),
            'NAH': judgment_data.get('NAH', 0)
        }
        
        return max(votes, key=votes.get)

    def run_experiment(self, max_posts: int = None) -> None:
        """Run the adversarial debate experiment."""
        print(f"\nðŸ”¬ EXPERIMENT 2: ADVERSARIAL REASONING & DEBATE")
        print("=" * 60)
        
        # Determine how many posts to process
        posts_to_process = len(self.data) if max_posts is None else min(max_posts, len(self.data))
        print(f"Analyzing {posts_to_process} posts with debate format...")
        
        successful_analyses = 0
        skipped_count = 0
        
        for i, post in enumerate(self.data[:posts_to_process]):
            print(f"\n[{i+1}/{posts_to_process}] Processing post {i+1}...")
            
            # Get post body
            body = post.get('body', '')
            if not body or len(body.strip()) < 50:
                skipped_count += 1
                print(f"  â­ï¸  SKIPPED: Post too short ({len(body.strip())} chars)")
                continue
            
            print(f"  ðŸ“ Post length: {len(body)} characters")
            
            # Get Reddit majority judgment
            reddit_judgment = self.get_reddit_majority_judgment(post)
            print(f"  ðŸ›ï¸  Reddit majority: {reddit_judgment}")
            
            # Stage 1: Get Pro arguments
            print(f"  ðŸ‘¤ Getting PRO arguments...")
            pro_arguments = self.get_pro_arguments(body)
            pro_quality = self.evaluate_argument_quality(pro_arguments, body)
            print(f"  âœ… PRO quality score: {pro_quality:.1f}/5.0")
            
            # Stage 2: Get Con arguments  
            print(f"  ðŸ‘¤ Getting CON arguments...")
            con_arguments = self.get_con_arguments(body)
            con_quality = self.evaluate_argument_quality(con_arguments, body)
            print(f"  âœ… CON quality score: {con_quality:.1f}/5.0")
            
            # Stage 3: Judge decision
            print(f"  âš–ï¸  Getting Judge decision...")
            judge_judgment, judge_explanation = self.get_judge_decision(body, pro_arguments, con_arguments)
            print(f"  ðŸŽ¯ Judge judgment: {judge_judgment}")
            
            # Calculate metrics
            debate_effectiveness = (pro_quality + con_quality) / 2
            convergence = reddit_judgment == judge_judgment
            convergence_symbol = "âœ…" if convergence else "âŒ"
            print(f"  ðŸ“Š Debate effectiveness: {debate_effectiveness:.1f}/5.0")
            print(f"  {convergence_symbol} Convergence with Reddit: {convergence}")
            
            # Get judgment data for vote counts
            if 'judgment_counts' in post:
                judgment_data = post['judgment_counts']
            else:
                judgment_data = post
            
            # Store results
            result = {
                'post_index': i,
                'body': body,
                'reddit_judgment': reddit_judgment,
                'pro_arguments': pro_arguments,
                'con_arguments': con_arguments,
                'pro_quality': pro_quality,
                'con_quality': con_quality,
                'debate_effectiveness': debate_effectiveness,
                'judge_judgment': judge_judgment,
                'judge_explanation': judge_explanation,
                'convergence': convergence,
                'reddit_votes': {
                    'NTA': judgment_data.get('NTA', 0),
                    'YTA': judgment_data.get('YTA', 0),
                    'ESH': judgment_data.get('ESH', 0),
                    'NAH': judgment_data.get('NAH', 0)
                }
            }
            
            self.results.append(result)
            successful_analyses += 1
            
            # Rate limiting
            time.sleep(2)  # Longer delay due to multiple API calls
        
        print(f"\nðŸŽ‰ Experiment 2 Complete!")
        print(f"âœ… Successful analyses: {successful_analyses}")
        print(f"â­ï¸  Skipped posts: {skipped_count}")
        print("=" * 60)

    def calculate_metrics(self) -> Dict:
        """Calculate experiment metrics."""
        if not self.results:
            return {}
        
        df = pd.DataFrame(self.results)
        
        # Core metrics
        convergence_rate = df['convergence'].mean()
        avg_debate_effectiveness = df['debate_effectiveness'].mean()
        avg_pro_quality = df['pro_quality'].mean()
        avg_con_quality = df['con_quality'].mean()
        
        # Distribution analysis
        reddit_dist = df['reddit_judgment'].value_counts(normalize=True)
        judge_dist = df['judge_judgment'].value_counts(normalize=True)
        
        # Convergence by judgment type
        convergence_by_type = df.groupby('reddit_judgment')['convergence'].mean()
        
        # Quality analysis
        quality_stats = {
            'pro_quality_std': df['pro_quality'].std(),
            'con_quality_std': df['con_quality'].std(),
            'effectiveness_std': df['debate_effectiveness'].std()
        }
        
        metrics = {
            'total_posts': len(df),
            'convergence_rate': convergence_rate,
            'avg_debate_effectiveness': avg_debate_effectiveness,
            'avg_pro_quality': avg_pro_quality,
            'avg_con_quality': avg_con_quality,
            'reddit_distribution': reddit_dist.to_dict(),
            'judge_distribution': judge_dist.to_dict(),
            'convergence_by_type': convergence_by_type.to_dict(),
            'quality_statistics': quality_stats
        }
        
        return metrics

    def create_visualizations(self) -> None:
        """Create visualizations for the experiment results."""
        if not self.results:
            print("No results to visualize.")
            return

        df = pd.DataFrame(self.results)
        
        # Set up plotting
        plt.style.use('default')
        sns.set_palette("husl")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Experiment 2: Adversarial Reasoning & Debate Results', fontsize=16, fontweight='bold')
        
        # 1. Convergence Rate
        convergence_rate = df['convergence'].mean()
        ax1 = axes[0, 0]
        ax1.pie([convergence_rate, 1-convergence_rate], 
                labels=[f'Convergence ({convergence_rate:.1%})', f'Divergence ({1-convergence_rate:.1%})'],
                autopct='%1.1f%%', startangle=90,
                colors=['#2ecc71', '#e74c3c'])
        ax1.set_title('Judge-Reddit Convergence Rate')
        
        # 2. Argument Quality Distribution
        ax2 = axes[0, 1]
        ax2.hist(df['pro_quality'], alpha=0.7, label='PRO Arguments', bins=10, density=True)
        ax2.hist(df['con_quality'], alpha=0.7, label='CON Arguments', bins=10, density=True)
        ax2.set_xlabel('Quality Score')
        ax2.set_ylabel('Density')
        ax2.set_title('Argument Quality Distribution')
        ax2.legend()
        
        # 3. Debate Effectiveness vs Convergence
        ax3 = axes[0, 2]
        converged = df[df['convergence'] == True]['debate_effectiveness']
        diverged = df[df['convergence'] == False]['debate_effectiveness']
        
        box_data = [converged.dropna(), diverged.dropna()]
        ax3.boxplot(box_data, labels=['Converged', 'Diverged'])
        ax3.set_ylabel('Debate Effectiveness Score')
        ax3.set_title('Effectiveness by Convergence')
        
        # 4. Judgment Distribution Comparison
        ax4 = axes[1, 0]
        reddit_counts = df['reddit_judgment'].value_counts()
        judge_counts = df['judge_judgment'].value_counts()
        
        judgments = ['NTA', 'YTA', 'ESH', 'NAH']
        reddit_values = [reddit_counts.get(j, 0) for j in judgments]
        judge_values = [judge_counts.get(j, 0) for j in judgments]
        
        x = np.arange(len(judgments))
        width = 0.35
        
        ax4.bar(x - width/2, reddit_values, width, label='Reddit', alpha=0.8)
        ax4.bar(x + width/2, judge_values, width, label='Judge', alpha=0.8)
        ax4.set_xlabel('Judgment Type')
        ax4.set_ylabel('Count')
        ax4.set_title('Judgment Distribution Comparison')
        ax4.set_xticks(x)
        ax4.set_xticklabels(judgments)
        ax4.legend()
        
        # 5. Pro vs Con Quality Scatter
        ax5 = axes[1, 1]
        scatter = ax5.scatter(df['pro_quality'], df['con_quality'], 
                             c=df['convergence'], cmap='RdYlGn', alpha=0.7)
        ax5.set_xlabel('PRO Argument Quality')
        ax5.set_ylabel('CON Argument Quality')
        ax5.set_title('PRO vs CON Quality (Color = Convergence)')
        plt.colorbar(scatter, ax=ax5)
        
        # 6. Convergence by Judgment Type
        ax6 = axes[1, 2]
        convergence_by_judgment = df.groupby('reddit_judgment')['convergence'].mean()
        
        bars = ax6.bar(convergence_by_judgment.index, convergence_by_judgment.values, alpha=0.7)
        ax6.set_xlabel('Reddit Judgment')
        ax6.set_ylabel('Convergence Rate')
        ax6.set_title('Convergence Rate by Judgment Type')
        ax6.set_ylim(0, 1)
        
        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.1%}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('experiment_2_results.png', dpi=300, bbox_inches='tight')
        plt.show()

    def print_summary(self) -> None:
        """Print a summary of the experiment results."""
        if not self.results:
            print("No results to summarize.")
            return

        metrics = self.calculate_metrics()
        
        print("\n" + "="*60)
        print("EXPERIMENT 2: ADVERSARIAL REASONING & DEBATE - SUMMARY")
        print("="*60)
        
        print(f"Total posts analyzed: {metrics['total_posts']}")
        print(f"Convergence Rate (C): {metrics['convergence_rate']:.3f} ({metrics['convergence_rate']:.1%})")
        print(f"Average Debate Effectiveness (D): {metrics['avg_debate_effectiveness']:.2f}/5.0")
        print(f"Average PRO Quality: {metrics['avg_pro_quality']:.2f}/5.0")
        print(f"Average CON Quality: {metrics['avg_con_quality']:.2f}/5.0")
        
        print(f"\nReddit judgment distribution:")
        for judgment, pct in metrics['reddit_distribution'].items():
            print(f"  {judgment}: {pct:.1%}")
        
        print(f"\nJudge judgment distribution:")
        for judgment, pct in metrics['judge_distribution'].items():
            print(f"  {judgment}: {pct:.1%}")
        
        print(f"\nConvergence by judgment type:")
        for judgment, rate in metrics['convergence_by_type'].items():
            print(f"  {judgment}: {rate:.1%}")
        
        # Show some examples of high-quality debates
        df = pd.DataFrame(self.results)
        high_quality = df.nlargest(2, 'debate_effectiveness')
        if not high_quality.empty:
            print(f"\nExample high-quality debates:")
            for _, row in high_quality.iterrows():
                print(f"  Effectiveness: {row['debate_effectiveness']:.1f}/5.0")
                print(f"  Reddit: {row['reddit_judgment']} | Judge: {row['judge_judgment']}")
                print(f"  Post snippet: {row['body'][:80]}...")
                print()

    def save_results(self, filename: str = 'experiment_2_results.json') -> None:
        """Save the experiment results to a JSON file."""
        if not self.results:
            print("No results to save.")
            return
        
        output_data = {
            'experiment': 'Adversarial Reasoning & Debate',
            'metrics': self.calculate_metrics(),
            'results': self.results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"Results saved to {filename}")


def main():
    """Main function to run Experiment 2."""
    # Initialize the experiment
    experiment = AdversarialDebateExperiment('judgment_analysis_results.json')
    
    # Run the experiment (processes all posts by default)
    experiment.run_experiment()
    
    # Print summary
    experiment.print_summary()
    
    # Create visualizations
    experiment.create_visualizations()
    
    # Save results
    experiment.save_results()


if __name__ == "__main__":
    main() 