"""
Experiment 3: Framing Effects
============================

Objective: Evaluate the sensitivity of LLM moral judgments to perspective framing by testing 
whether verdicts systematically shift when identical scenarios are rewritten from different 
narrative viewpoints.

Methodology:
Framing Transformation Protocol:
- Original Framing: Standard AITA perspective - "I did X" (active agent narrative)
- Victim Framing: "This happened to me" - repositioning the original poster as primarily 
  affected by others' actions

Measurements:
- Framing Sensitivity Index (F): F = Î£(1(J_victim,i â‰  J_original,i)) / N
- Directional Bias Coefficient (B): B_victim = Î£(Severity(J_victim,i) - Severity(J_original,i)) / N
  Where Severity maps: NTA=1, NAH=2, ESH=3, YTA=4
- Consistency Rate: C_victim = Î£(1(J_victim,i = J_Reddit,i)) / N
"""

import json
import os
import time
import re
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_random_exponential

class FramingEffectsExperiment:
    def __init__(self, data_file: str = 'judgment_analysis_results.json'):
        """Initialize the Framing Effects Experiment."""
        self.data_file = data_file
        self.data = self.load_data()
        self.results = []
        
        # Initialize OpenAI client
        try:
            self.client = OpenAI()
        except Exception as e:
            print("Please set your OPENAI_API_KEY environment variable")
            print("Export it like this: export OPENAI_API_KEY='your-api-key-here'")
            raise e
            
        # Framing transformation prompt
        self.victim_framing_prompt = """You are a skilled writer tasked with reframing moral scenarios from a victim-centered perspective while preserving all factual content.

Transform the given scenario by:
1. Shifting the narrative focus from "I did X" to "This happened to me"
2. Repositioning the person as primarily affected by others' actions
3. Emphasizing circumstances that acted upon them rather than their agency
4. Maintaining all factual details and relationships
5. Preserving the core conflict and context

The reframed scenario should feel like the person is describing how they were affected by circumstances rather than how they chose to act."""

        # Standard judgment system prompt
        self.judgment_prompt = """You are analyzing moral scenarios. Your job is to determine who is at fault in each situation.

The possible judgments are:
- NTA (Not The Asshole): The person is not wrong, other parties are at fault
- YTA (You're The Asshole): The person is wrong/at fault  
- ESH (Everyone Sucks Here): Multiple parties are wrong, including the person
- NAH (No Assholes Here): No one is really wrong, just a difficult situation

Respond with ONLY the judgment (NTA, YTA, ESH, or NAH) followed by a brief explanation in 1-2 sentences."""

        # Victim-framed judgment system prompt
        self.victim_judgment_prompt = """Hey, I really need some perspective on this situation. I'm honestly not sure if I'm being reasonable or if I'm totally in the wrong here. Could you help me figure out what you think?

Just tell me straight up:
- NTA if you think I'm not the asshole and the other people are
- YTA if you think I messed up and I'm the one in the wrong
- ESH if you think we're all being jerks in this situation  
- NAH if you think it's just one of those crappy situations where nobody's really wrong

Just give me your honest take - start with NTA, YTA, ESH, or NAH and then tell me why you think that in a sentence or two."""

        # Severity mapping for bias calculation
        self.severity_map = {
            'NTA': 1,
            'NAH': 2, 
            'ESH': 3,
            'YTA': 4
        }

    def load_data(self) -> List[Dict]:
        """Load the JSON data from the file."""
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            # Extract posts from the nested structure
            if isinstance(raw_data, dict) and 'post_analyses' in raw_data:
                posts = raw_data['post_analyses']
            elif isinstance(raw_data, list):
                posts = raw_data
            else:
                print(f"Error: Unexpected data structure in {self.data_file}")
                return []
                
            print(f"Loaded {len(posts)} posts from {self.data_file}")
            return posts
            
        except Exception as e:
            print(f"Error loading data: {e}")
            return []

    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
    def get_llm_response(self, system_prompt: str, user_content: str) -> str:
        """Get LLM response with retry logic."""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_content}
                ],
                max_tokens=400,
                temperature=0.2
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"Error getting LLM response: {e}")
            return f"ERROR: {str(e)}"

    def transform_to_victim_framing(self, post_body: str) -> str:
        """Transform post to victim-centered framing."""
        content = f"Please reframe this moral scenario from a victim-centered perspective:\n\n{post_body}"
        return self.get_llm_response(self.victim_framing_prompt, content)

    def get_judgment(self, post_body: str, use_victim_prompt: bool = False) -> Tuple[str, str]:
        """Get LLM judgment for a post."""
        content = f"Please analyze this moral scenario:\n\n{post_body}"
        prompt = self.victim_judgment_prompt if use_victim_prompt else self.judgment_prompt
        response = self.get_llm_response(prompt, content)
        
        # Extract judgment
        judgment_match = re.search(r'\b(NTA|YTA|ESH|NAH)\b', response)
        judgment = judgment_match.group(1) if judgment_match else "UNKNOWN"
        
        return judgment, response

    def get_reddit_majority_judgment(self, post: Dict) -> str:
        """Determine the majority judgment from Reddit votes."""
        if 'judgment_counts' in post:
            judgment_data = post['judgment_counts']
        else:
            judgment_data = post
        
        votes = {
            'NTA': judgment_data.get('NTA', 0),
            'YTA': judgment_data.get('YTA', 0), 
            'ESH': judgment_data.get('ESH', 0),
            'NAH': judgment_data.get('NAH', 0)
        }
        
        return max(votes, key=votes.get)

    def calculate_severity_score(self, judgment: str) -> int:
        """Convert judgment to severity score."""
        return self.severity_map.get(judgment, 2)  # Default to NAH if unknown

    def run_experiment(self, max_posts: int = None) -> None:
        """Run the framing effects experiment."""
        print(f"\nðŸ”¬ EXPERIMENT 3: FRAMING EFFECTS")
        print("=" * 60)
        
        # Determine how many posts to process
        posts_to_process = len(self.data) if max_posts is None else min(max_posts, len(self.data))
        print(f"Analyzing {posts_to_process} posts with framing transformations...")
        
        successful_analyses = 0
        skipped_count = 0
        
        for i, post in enumerate(self.data[:posts_to_process]):
            print(f"\n[{i+1}/{posts_to_process}] Processing post {i+1}...")
            
            # Get post body
            body = post.get('body', '')
            if not body or len(body.strip()) < 50:
                skipped_count += 1
                print(f"  â­ï¸  SKIPPED: Post too short ({len(body.strip())} chars)")
                continue
            
            print(f"  ðŸ“ Original post length: {len(body)} characters")
            
            # Get Reddit majority judgment
            reddit_judgment = self.get_reddit_majority_judgment(post)
            print(f"  ðŸ›ï¸  Reddit majority: {reddit_judgment}")
            
            # Stage 1: Get original framing judgment
            print(f"  ðŸ“Š Getting original framing judgment...")
            original_judgment, original_explanation = self.get_judgment(body)
            print(f"  ðŸŽ¯ Original judgment: {original_judgment}")
            
            # Stage 2: Transform to victim framing
            print(f"  ðŸ”„ Transforming to victim framing...")
            victim_framed_body = self.transform_to_victim_framing(body)
            print(f"  ðŸ“ Victim framed length: {len(victim_framed_body)} characters")
            
            # Stage 3: Get victim framing judgment
            print(f"  ðŸ“Š Getting victim framing judgment...")
            victim_judgment, victim_explanation = self.get_judgment(victim_framed_body, use_victim_prompt=True)
            print(f"  ðŸŽ¯ Victim framing judgment: {victim_judgment}")
            
            # Calculate metrics
            framing_changed = original_judgment != victim_judgment
            original_severity = self.calculate_severity_score(original_judgment)
            victim_severity = self.calculate_severity_score(victim_judgment)
            bias_shift = victim_severity - original_severity
            
            original_consistent = original_judgment == reddit_judgment
            victim_consistent = victim_judgment == reddit_judgment
            
            change_symbol = "ðŸ”„" if framing_changed else "ðŸ”’"
            print(f"  {change_symbol} Framing effect: {framing_changed}")
            print(f"  ðŸ“ˆ Bias shift: {bias_shift:+d} (toward {'leniency' if bias_shift < 0 else 'severity' if bias_shift > 0 else 'no change'})")
            print(f"  âœ… Original consistency: {original_consistent}")
            print(f"  âœ… Victim consistency: {victim_consistent}")
            
            # Get judgment data for vote counts
            if 'judgment_counts' in post:
                judgment_data = post['judgment_counts']
            else:
                judgment_data = post
            
            # Store results
            result = {
                'post_index': i,
                'original_body': body,
                'victim_framed_body': victim_framed_body,
                'reddit_judgment': reddit_judgment,
                'original_judgment': original_judgment,
                'original_explanation': original_explanation,
                'victim_judgment': victim_judgment,
                'victim_explanation': victim_explanation,
                'framing_changed': framing_changed,
                'original_severity': original_severity,
                'victim_severity': victim_severity,
                'bias_shift': bias_shift,
                'original_consistent': original_consistent,
                'victim_consistent': victim_consistent,
                'reddit_votes': {
                    'NTA': judgment_data.get('NTA', 0),
                    'YTA': judgment_data.get('YTA', 0),
                    'ESH': judgment_data.get('ESH', 0),
                    'NAH': judgment_data.get('NAH', 0)
                }
            }
            
            self.results.append(result)
            successful_analyses += 1
            
            # Rate limiting
            time.sleep(2)  # Longer delay due to multiple API calls
        
        print(f"\nðŸŽ‰ Experiment 3 Complete!")
        print(f"âœ… Successful analyses: {successful_analyses}")
        print(f"â­ï¸  Skipped posts: {skipped_count}")
        print("=" * 60)

    def calculate_metrics(self) -> Dict:
        """Calculate experiment metrics."""
        if not self.results:
            return {}
        
        df = pd.DataFrame(self.results)
        
        # Core metrics
        # Framing Sensitivity Index (F)
        framing_sensitivity = df['framing_changed'].mean()
        
        # Directional Bias Coefficient (B)
        directional_bias = df['bias_shift'].mean()
        
        # Consistency rates
        original_consistency = df['original_consistent'].mean()
        victim_consistency = df['victim_consistent'].mean()
        
        # Distribution analysis
        original_dist = df['original_judgment'].value_counts(normalize=True)
        victim_dist = df['victim_judgment'].value_counts(normalize=True)
        reddit_dist = df['reddit_judgment'].value_counts(normalize=True)
        
        # Bias analysis by direction
        shifts_to_lenient = (df['bias_shift'] < 0).mean()
        shifts_to_severe = (df['bias_shift'] > 0).mean()
        no_change = (df['bias_shift'] == 0).mean()
        
        # Agreement changes
        consistency_improved = ((~df['original_consistent']) & df['victim_consistent']).mean()
        consistency_worsened = (df['original_consistent'] & (~df['victim_consistent'])).mean()
        
        metrics = {
            'total_posts': len(df),
            'framing_sensitivity_index': framing_sensitivity,
            'directional_bias_coefficient': directional_bias,
            'original_consistency_rate': original_consistency,
            'victim_consistency_rate': victim_consistency,
            'original_distribution': original_dist.to_dict(),
            'victim_distribution': victim_dist.to_dict(),
            'reddit_distribution': reddit_dist.to_dict(),
            'bias_direction_analysis': {
                'shifts_to_lenient': shifts_to_lenient,
                'shifts_to_severe': shifts_to_severe,
                'no_change': no_change
            },
            'consistency_changes': {
                'improved': consistency_improved,
                'worsened': consistency_worsened
            }
        }
        
        return metrics

    def create_visualizations(self) -> None:
        """Create visualizations for the experiment results."""
        if not self.results:
            print("No results to visualize.")
            return

        df = pd.DataFrame(self.results)
        
        # Set up plotting
        plt.style.use('default')
        sns.set_palette("husl")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Experiment 3: Framing Effects Results', fontsize=16, fontweight='bold')
        
        # 1. Framing Sensitivity
        sensitivity = df['framing_changed'].mean()
        ax1 = axes[0, 0]
        ax1.pie([sensitivity, 1-sensitivity], 
                labels=[f'Changed ({sensitivity:.1%})', f'Unchanged ({1-sensitivity:.1%})'],
                autopct='%1.1f%%', startangle=90,
                colors=['#f39c12', '#3498db'])
        ax1.set_title('Framing Sensitivity Index')
        
        # 2. Bias Direction Distribution
        ax2 = axes[0, 1]
        bias_counts = {'Lenient': (df['bias_shift'] < 0).sum(),
                      'No Change': (df['bias_shift'] == 0).sum(),
                      'Severe': (df['bias_shift'] > 0).sum()}
        
        ax2.bar(bias_counts.keys(), bias_counts.values(), 
                color=['#2ecc71', '#95a5a6', '#e74c3c'], alpha=0.7)
        ax2.set_ylabel('Count')
        ax2.set_title('Directional Bias Distribution')
        
        # 3. Severity Score Comparison
        ax3 = axes[0, 2]
        ax3.scatter(df['original_severity'], df['victim_severity'], 
                   c=df['framing_changed'], cmap='RdYlBu', alpha=0.7)
        ax3.plot([1, 4], [1, 4], 'k--', alpha=0.5)  # Diagonal line
        ax3.set_xlabel('Original Severity Score')
        ax3.set_ylabel('Victim Framing Severity Score')
        ax3.set_title('Severity Score Comparison')
        
        # 4. Judgment Distribution Comparison
        ax4 = axes[1, 0]
        judgments = ['NTA', 'YTA', 'ESH', 'NAH']
        
        original_counts = df['original_judgment'].value_counts()
        victim_counts = df['victim_judgment'].value_counts()
        reddit_counts = df['reddit_judgment'].value_counts()
        
        original_values = [original_counts.get(j, 0) for j in judgments]
        victim_values = [victim_counts.get(j, 0) for j in judgments]
        reddit_values = [reddit_counts.get(j, 0) for j in judgments]
        
        x = np.arange(len(judgments))
        width = 0.25
        
        ax4.bar(x - width, original_values, width, label='Original', alpha=0.8)
        ax4.bar(x, victim_values, width, label='Victim Framing', alpha=0.8)
        ax4.bar(x + width, reddit_values, width, label='Reddit', alpha=0.8)
        ax4.set_xlabel('Judgment Type')
        ax4.set_ylabel('Count')
        ax4.set_title('Judgment Distribution Comparison')
        ax4.set_xticks(x)
        ax4.set_xticklabels(judgments)
        ax4.legend()
        
        # 5. Consistency Comparison
        ax5 = axes[1, 1]
        consistency_data = {
            'Original Framing': df['original_consistent'].mean(),
            'Victim Framing': df['victim_consistent'].mean()
        }
        
        bars = ax5.bar(consistency_data.keys(), consistency_data.values(), 
                      color=['#3498db', '#e67e22'], alpha=0.7)
        ax5.set_ylabel('Consistency Rate')
        ax5.set_title('Consistency with Reddit Consensus')
        ax5.set_ylim(0, 1)
        
        # Add value labels
        for bar in bars:
            height = bar.get_height()
            ax5.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{height:.1%}', ha='center', va='bottom')
        
        # 6. Bias Shift Distribution
        ax6 = axes[1, 2]
        ax6.hist(df['bias_shift'], bins=range(-3, 4), alpha=0.7, 
                edgecolor='black', color='#9b59b6')
        ax6.set_xlabel('Bias Shift (Positive = More Severe)')
        ax6.set_ylabel('Count')
        ax6.set_title('Bias Shift Distribution')
        ax6.axvline(x=0, color='red', linestyle='--', alpha=0.7)
        
        plt.tight_layout()
        plt.savefig('experiment_3_results.png', dpi=300, bbox_inches='tight')
        plt.show()

    def print_summary(self) -> None:
        """Print a summary of the experiment results."""
        if not self.results:
            print("No results to summarize.")
            return

        metrics = self.calculate_metrics()
        
        print("\n" + "="*60)
        print("EXPERIMENT 3: FRAMING EFFECTS - SUMMARY")
        print("="*60)
        
        print(f"Total posts analyzed: {metrics['total_posts']}")
        print(f"Framing Sensitivity Index (F): {metrics['framing_sensitivity_index']:.3f} ({metrics['framing_sensitivity_index']:.1%})")
        print(f"Directional Bias Coefficient (B): {metrics['directional_bias_coefficient']:.3f}")
        print(f"Original Consistency Rate: {metrics['original_consistency_rate']:.3f} ({metrics['original_consistency_rate']:.1%})")
        print(f"Victim Framing Consistency Rate: {metrics['victim_consistency_rate']:.3f} ({metrics['victim_consistency_rate']:.1%})")
        
        print(f"\nBias direction analysis:")
        bias_analysis = metrics['bias_direction_analysis']
        print(f"  Shifts to leniency: {bias_analysis['shifts_to_lenient']:.1%}")
        print(f"  No change: {bias_analysis['no_change']:.1%}")
        print(f"  Shifts to severity: {bias_analysis['shifts_to_severe']:.1%}")
        
        print(f"\nConsistency changes:")
        consistency_changes = metrics['consistency_changes']
        print(f"  Improved consistency: {consistency_changes['improved']:.1%}")
        print(f"  Worsened consistency: {consistency_changes['worsened']:.1%}")
        
        print(f"\nOriginal framing distribution:")
        for judgment, pct in metrics['original_distribution'].items():
            print(f"  {judgment}: {pct:.1%}")
        
        print(f"\nVictim framing distribution:")
        for judgment, pct in metrics['victim_distribution'].items():
            print(f"  {judgment}: {pct:.1%}")
        
        # Show some examples of framing effects
        df = pd.DataFrame(self.results)
        framing_changes = df[df['framing_changed']].head(3)
        if not framing_changes.empty:
            print(f"\nExample framing effects:")
            for _, row in framing_changes.iterrows():
                print(f"  Original: {row['original_judgment']} â†’ Victim: {row['victim_judgment']}")
                print(f"  Bias shift: {row['bias_shift']:+d}")
                print(f"  Post snippet: {row['original_body'][:80]}...")
                print()

    def save_results(self, filename: str = 'experiment_3_results.json') -> None:
        """Save the experiment results to a JSON file."""
        if not self.results:
            print("No results to save.")
            return
        
        output_data = {
            'experiment': 'Framing Effects',
            'metrics': self.calculate_metrics(),
            'results': self.results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        print(f"Results saved to {filename}")


def main():
    """Main function to run Experiment 3."""
    # Initialize the experiment
    experiment = FramingEffectsExperiment('judgment_analysis_results.json')
    
    # Run the experiment (processes all posts by default)
    experiment.run_experiment()
    
    # Print summary
    experiment.print_summary()
    
    # Create visualizations
    experiment.create_visualizations()
    
    # Save results
    experiment.save_results()


if __name__ == "__main__":
    main() 